\section{Approach} 
\label{approach}

TODO: Pipeline image

\subsection{Dataset Creation}  \label{ch:approachA}
% Choosing latest mutations that are the most spread
% Created three types of sequence pairs: (real, real), (real, generated), (real, unreal)
% Phylogenetic tree with "Fasttree"
% Levenshtein distance
% Biopython packages?
% Final dataset metrics

hier irgendwo das Zielschema des Datensatzes beschreiben


\subsubsection{Raw data selection from GISAID}
\label{ch:approachAa}

focus: Germany from 4.5. - 6.8. (new variant arised in the recent past -> lambda, delta, ...)
only Germany, to make it possible to handle the data
about 35000 genomes in our raw dataset

beispiel record: genome sequence and metadata


\subsubsection{Generation of a phylogenetic tree}
\label{ch:approachAb}


\subsubsection{Phylogenetic tree to dataset}
\label{ch:approachAc}


\subsection{Data Preprocessing}  \label{ch:approachB}
% TODO: Pipeline image
% Word size 3 is just a hyperparameter (see discussion of the corresponding paper)!  But maybe biologically valid because auf amino acids.
% DNA2Vec?

two steps:
- to make the dimensionality managable not the whole 30000 nucleotides are evaluated. We take a subpart of X nucleotides from position A to B
- Transform string to numeric for model input

\subsubsection{Dimensionality reduction by selecting subpart of the genome}
\label{ch:approachBa}


\subsubsection{Transform genome sequence to numeric model input}
\label{ch:approachBb}


\begin{itemize}
	\item DNA Sequencing (Done during dataset creation, given from GISAID)
	\item DNA Sequence Tokenization for Amino Acid Dictionary
	\item DNA Sequence Padding
\end{itemize}

\subsection{Model Architecture}  \label{ch:approachC}
% Hyperparameters and model architecture
% Reference http://nlp.seas.harvard.edu/2018/04/03/attention.html if one wants to build it by oneself

\subsection{Training Process} \label{ch:approachD}
% Plot accuracy and loss for training and validation
% Loss functions and teacher forcing, early stopping?
% Categorical cross-entropy loss for seq2seq, Wasserstein loss for GAN, kullback leibner + cross entropy for transformers
% Identify changes between sequences using the diff-match package from Google
% See MutaGAN 6.2 Model training
% Replay buffer
% Dealing with the mode collapse problem

\newpage
