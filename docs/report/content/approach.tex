\section{Approach} 
\label{approach}

Figure \ref{pipeline} visualizes the steps of our approach, which are described in detail in the following chapters.

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/pipeline.png}
	\caption{Approach pipeline \cite{own representation}}
	\label{pipeline}
\end{figure}


\subsection{Dataset Creation}  \label{ch:approachA}
% Choosing latest mutations that are the most spread
% Created three types of sequence pairs: (real, real), (real, generated), (real, unreal)
% Phylogenetic tree with "Fasttree"
% Levenshtein distance
% Biopython packages?
% Final dataset metrics

As described in chapter \ref{fundamentalsC} a dataset consisting of parent-child pairs is needed to learn a \ac{ML} model for mutation prediction. The steps to create this dataset are described in the following chapters.

\subsubsection{Raw data selection from \ac{GISAID}}
\label{ch:approachAa}

In the first step of the dataset creation the raw genomes and their metadata are downloaded from \ac{GISAID}. Therefore a selection of a suitable subpart of the data is necessary. The \ac{GISAID} platform only allows the download of 5000 genomes at once. That is why we decided to focus our analysis on Germany (for one country the selection of < 5000 genomes is rather simple compared to the selection of data from multiple countries). By looking at the latest "Report on virus variants of SARS-CoV-2 in Germany" from the \ac{RKI} we decided for a time period. Figure \ref{rkiVariantDistribution} shows the distribution of different variants over time. Starting from calendar week 18 the Delta variant gradually displaces the previously widespread Alpha variant.

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/rkiVariantDistribution.png}
	\caption{Variant distribution in Germany over time \cite{robertkochinstituteditorBerichtVirusvariantenSARSCoV22021}}
	\label{rkiVariantDistribution}
\end{figure}

That is why we selected the data from 04.05.2021 (calendar week 18) until 15.07.2021 (calendar week 28). This results in a raw dataset of 35.818 genomes.
Each genome consists of a FASTA file containing the genomes sequence and a record in a tsv file with the metadata.

%TODO: beispiel record: genome sequence and metadata


\subsubsection{Generation of a phylogenetic tree}
\label{ch:approachAb}

As described above we need parent-child genome sequence pairs to be able to train a \ac{ML} model. Therefore we need to evaluate the ancestral relationships between the genome sequences.
As described in chapter \ref{fundamentalsA0d} this is done in biology through phylogenetic trees.
For the data preparation (e.g. alignment) and the calculation of the phylogenetic tree we use the nextstrain pipeline \cite{10.1093/bioinformatics/bty407}.
The pipeline was executed on a local computer with Intel Core i7-7500U (4*2.70GHz), NVIDIA GeForce 940MX and 16 GB RAM.
Unfortunately after two days of calculation, the nextstrain pipeline exited with an out of memory exception. That is why the amount of data is decreased. This was done through iterative reduction of the dataset size. The largest locally computable dataset consists of 11.773 genome sequences.

The generated phylogenetic tree can be seen in figure \ref{phylogeneticTree}.
%TODO: Beschreibung was man sieht

%TODO: nicer tree image?
\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/phylogeneticTree.jpg}
	\caption{Phylogenetic tree of our dataset \cite{own representation}}
	\label{phylogeneticTree}
\end{figure}

\subsubsection{Phylogenetic tree to dataset}
\label{ch:approachAc}

Based on the calculated phylogenetic tree the final dataset is generated. Therefore the patristic distances between all leaf nodes are calculated. From the resulting patristic distance matrix for each leaf node the most related next leaf node can be evaluated. The two leaf nodes with the smallest distance are the closest relatives to each other and therefore represent a parent-child pair. After the evaluation which leaf nodes are one parent-child pair, the question which node is the parent and which node is the child is clarified. Therefore the two nodes are sorted by their date, which is stored in the metadata file. The older genome sequence is the parent and the younger genome sequence is the child.

For calculating the patristic distance matrix we first used Dendropy (version: 4.5.2) \cite{DendroPyPhylogeneticComputing}. Again this leads to problems due to limited RAM on our local computer. As a consequence we switched to PhyloDM (version: 1.3.1) \cite{aaron_mussig_2020_4089111}, which is a library for calculating patristic distances in a memory and time efficient manner.

Finally we received a dataset containing parent-child genome sequences.

\subsection{Data Preprocessing}  \label{ch:approachB}
% TODO: Pipeline image
% Word size 3 is just a hyperparameter (see discussion of the corresponding paper)!  But maybe biologically valid because auf amino acids.

\subsubsection{Dimensionality reduction by selecting subpart of the genome}
\label{ch:approachBa}

The full \ac{SARS-CoV-2} genome consists of about 30.000 nucleotides. Due to the limited computing capacity on our local computers we need to reduce the dimensionality of the dataset. This is done by selecting a subpart based on the gained data insights from chapter \ref{ch:experimentsA}. In figure \ref{mutatedGeneticLoci} parts of the genome with lots of mutations are visible. Based upon this we selected 99 nucleotides (33 codons) in the range between position 21.800 and 21.899.

\subsubsection{Transform genome sequence to numeric model input}
\label{ch:approachBb}

Input to the model are numericalized codon sequences with each codon being composed of three nucleotides. Therefore one needs to build a vocabulary with all codons existing in the dataset that assigns each codon a unique number. Each genome string is therefore tokenized to codons and so far unseen codons are added to the vocabulary. The vocabulary also contains special tokens like:

\begin{itemize}
	\item \textit{<SOS>}: start of sentence
	\item \textit{<EOS>}: end of sentence
	\item \textit{<UNK>}: unknown
	\item \textit{<PAD>}: padding
\end{itemize}

A codon is encoded as unknown in case at least one nulceotide is unknown or at least one padding character is contained. Only in the special case of three padding characters the padding token is returned. 

To integrate the dataset into the training routine, PyTorchs \textit{DataLoader} is used (see \autoref{ch:approachD}). Therefore a \textit{CustomGISAIDDataset} class is derived from PyTorchs Dataset class that can be utilized by the \textit{DataLoader}. It can be configured to return the training or test dataset instances. Each instance is already numericalized by vocabulary index lookups for each codon, padded with the numericalized \textit{<SOS>} and \textit{<EOS>} token and transformed to a PyTorch tensor. 

\subsection{Model Architecture}  \label{ch:approachC}

The connection of modeling evolution theory of virus mutations to probabilistic language modeling has already been explained in \autoref{fundamentalsA}. To create such a probabilistic language model an approach closely related to the one introduced by the MutaGAN paper \cite{Berman2020} as explained in \autoref{fundamentalsF} is chosen with the novelty that attention-based transformers are used instead of \acp{LSTM}\footnote{This improvement was also named in the conclusion of the MutaGAN paper \cite{Berman2020}.}. To implement the \ac{GAN} framework, PyTorch version 1.9.0 is used as it provides full-featured classes for transformers, embedding layers and other architectural components needed compared to other frameworks like Keras. This way one can specialize on the training routine instead of implementing a custom transformer architecture, which might be too error-prone\footnote{In case one is interested in how to implement a custom transformer architecture, see \url{http://nlp.seas.harvard.edu/2018/04/03/attention.html}.}. 

The architecture of the generator was shown in \autoref{transformer}. The numericalized codon sequences of the source parent and the target child sequence first need to be transformed to input embeddings of size 32 for each codon. This size was chosen experimentally and seemed to work best in practice as it only needs to model a vocabulary of size 40 for the parent vocabulary and 40 for the child vocabulary. To those input embeddings positional encodings are added that are formed by embedding each index in a sequence of positional indices for each instance. To both input sequences dropout layers are applied with a dropout rate of 10\%. The transformer then predicts the child sequence as a hidden representation of dimensionality 64 for each codon. The hidden representation is the output of the feed forward layer in a transformer block, its dimensionality was also chosen according to best experimental performance. In total three encoder and decoder layers are used each having eight heads. The transformer also uses a dropout rate of 10\%. The final feed forward network reduces the dimensionality from 64 back to the size of the vocabulary for each codon and applies the softmax function to receive the final output probabilities. Note that padding tokens or so far unpredicted child tokens are masked away to not be considered when calculating the loss during the training routine.

The architecture of the discriminator is kept similar to \autoref{mutagan} using PyTorch's \textit{TransformerEncoders} instead of \acp{LSTM}. Again one transforms the numericalized parent and either true or generated child sequence to embeddings of size 32 of each codon and adds the positional embeddings before applying dropout. Instead of using an embedding layer for the true or generated child sequence one uses a dense layer instead in order to directly feed in the probability distribution of generated child sequences. The true child sequence therefore needs to be one-hot encoded before being provided to the discriminator in order to match the shape. The transformer encoder then produces an encoding of dimensionality 64 for each codon. During the training phase dropout is once again used with a rate of 10\%. Note that until this step the same weights are used as in the generator. In the following step the two hidden representations of the parent and true or generated child sequence are concatenated along the embedding dimension. The subsequent feed-forward network uses three blocks of dropout, batch normalization and a linear layer with \ac{ReLU} activation. The \ac{ReLU} layer in the last block is replaced with a sigmoid layer that makes sure each codon receives a score between zero and one that states how likely its a true or generated codon. Once again padding tokens are masked away during the training phase.

\subsection{Training Process} \label{ch:approachD}

The training phase is two-fold and is performed on a single GeForce GTX 1080 \ac{GPU}. First a pre-training phase for the generator needs to be performed in order to produce first reasonable child sequences. Also the weights of the embedding layers and the encoder part of the generator will be reused for the discriminator in the second training phase. For the pre-training phase the cross-entropy loss function typical for language modeling is used, the learning rate is set to 0.01 aligned with \cite{Berman2020}. Teacher forcing is used in the initial pre-training phase to accelerate the training process by leading the generator into a right direction towards reasonable child sequences. In the main training phase the generator and discriminator compete against each other. When training the discriminator, the generator first generates a child sequence given a parent sequence. The true and generated child sequence is then given to the discriminator. Here the binary cross entropy loss is used to model the two parts of the classical \ac{GAN} loss function: 

\begin{equation}
	\underset{G}{arg min} \; \underset{D}{arg max} \; E_{x \sim p*(x)}(log(D(x))) + E_{z \sim p(z)}(log(1-D(G(z))))
\end{equation}

PyTorch's \textit{BCELoss} function is given as:

\begin{equation}
	l = -w(y\,log(x)) + (1-y)\,log(1-x)
\end{equation}

We therefore pass $y=1$ when calculating the left part of the loss using the true child sequence and $y=0$ when calculating the right part of the loss using the generated child sequence. We use the same mechanism to calculate the loss for the generator, but maximize $log(D(G(z)))$ instead of minimizing $log(1-D(G(z)))$. After experiencing mode collapse problems the MutaGAN paper \cite{Berman2020} switched to the Wasserstein loss function instead of the binary cross entropy function and therefore also changed the last layer of the discriminator to be a linear activation function. As PyTorch does not offer a Wasserstein loss function the binary cross entropy loss function was kept. The learning rate for the generator was set to 0.001 whereas the learning rate for the discriminator was set far lower to 0.00003 to avoid mode collapse. Besides providing true child sequences and generated child sequences to the discriminator, the MutaGAN paper \cite{Berman2020} also provides unrelated, but true child sequences to the discriminator. This way it is hoped to not only learn which mutations are relevant as inherently given by the training dataset, but also to strengthen the knowledge which mutations are especially related to a specific parent sequence and which not. For time reasons the training dataset was not enriched with such true but unrelated child sequences. 

Both training phases use batch gradient descent as their optimization method with a batch size of 32. Gradients are clipped to a maximal norm of one to avoid exploding gradients. Furthermore for both training phases the Adam optimizer is used with an initial learning rate of $3*10^{-4}$. A learning rate scheduler makes sure to reduce the learning rate by a factor of ten in case the loss has not improved for ten epochs. On every batch gradient descent a tensorboard is updated to draw the loss curve. Every ten epochs a model checkpoint is saved. Pretraining is done for only 10 epochs whereas the main training phase runs for 500 epochs. 

% Categorical cross-entropy loss for seq2seq, Wasserstein loss for GAN, kullback leibner + cross entropy for transformers

\newpage
