\section{Approach} 
\label{approach}

Figure \ref{pipeline} visualizes the steps of our approach, which are described in detail in the following chapters.

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/pipeline.png}
	\caption{Approach pipeline \cite{own representation}}
	\label{pipeline}
\end{figure}


\subsection{Dataset Creation} \label{approachA}

As described in chapter \ref{fundamentalsD} a dataset consisting of parent-child genome pairs is needed to learn a machine learning model for mutation prediction. The steps to create this dataset are described in the following chapters.

\subsubsection{Raw Data Selection from \ac{GISAID}} \label{approachAa}

In the first step of the dataset creation, the raw genomes and their metadata are downloaded from \ac{GISAID}. Therefore, a selection of a suitable subpart of the data is necessary. The \ac{GISAID} platform only allows the download of 5000 genomes at once. That is why the decision was made to focus the analysis on Germany (for one country the selection of < 5000 genomes is rather simple compared to the selection of data from multiple countries). By looking at the latest "Report on virus variants of SARS-CoV-2 in Germany" from the \ac{RKI} a time period was chosen. Figure \ref{rkiVariantDistribution} shows the distribution of different variants over time. Starting from calendar week 18 the Delta variant gradually displaces the previously widespread Alpha variant.

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/rkiVariantDistribution.png}
	\caption{Variant distribution in Germany over time \cite{robertkochinstituteditorBerichtVirusvariantenSARSCoV22021}}
	\label{rkiVariantDistribution}
\end{figure}

That is why the data from 04.05.2021 (calendar week 18) until 15.07.2021 (calendar week 28) was selected for this study. This results in a raw dataset of 35.818 genomes. Each genome consists of a FASTA file containing the genomes sequence and a record in a .tsv file with the metadata.

%TODO: beispiel record: genome sequence and metadata

\subsubsection{Generation of a Phylogenetic Tree} \label{approachAb}

As described above parent-child genome sequence pairs are needed to be able to train the model. Therefore one needs to evaluate the ancestral relationships between the genome sequences. As described in chapter \ref{fundamentalsAd} this is done in biology through phylogenetic trees. For the data preparation (e.g. alignment) and the calculation of the phylogenetic tree the nextstrain pipeline is used \cite{10.1093/bioinformatics/bty407}. The pipeline was executed on a local computer with Intel Core i7-7500U (4*2.70GHz), NVIDIA GeForce 940MX and 16 GB RAM. Unfortunately, after two days of calculation, the nextstrain pipeline exited with an out-of-memory exception. That is why the amount of data was decreased. This was done through iterative reduction of the dataset size. The largest locally computable dataset consists of 11.773 genome sequences.

The generated phylogenetic tree can be seen in figure \ref{phylogeneticTree}.
%TODO: Beschreibung was man sieht

%TODO: nicer tree image?
\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/phylogeneticTree.jpg}
	\caption{Phylogenetic tree of our dataset \cite{own representation}}
	\label{phylogeneticTree}
\end{figure}

\subsubsection{Phylogenetic Tree to Dataset} \label{approachAc}

Based on the calculated phylogenetic tree the final dataset is generated. Therefore the patristic distances between all leaf nodes are calculated. From the resulting patristic distance matrix for each leaf node, the most related next leaf node can be evaluated. The two leaf nodes with the smallest distance are the closest relatives to each other and therefore represent a parent-child pair. After the evaluation of which leaf nodes are one parent-child genome pair, the question of which node is the parent and which node is the child is clarified. Therefore the two nodes are sorted by their date, which is stored in the metadata file. The older genome sequence is the parent and the younger genome sequence is the child.

For calculating the patristic distance matrix Dendropy (version: 4.5.2) \cite{DendroPyPhylogeneticComputing} was first used. Again this led to problems due to limited RAM on the local computer. As a consequence, a switch to PhyloDM (version: 1.3.1) \cite{aaron_mussig_2020_4089111} was done, which is a library for calculating patristic distances in a memory and time-efficient manner. Finally, a dataset containing parent-child genome sequences was received.

\subsection{Data Preprocessing} \label{approachB}

\subsubsection{Dimensionality Reduction by selecting Subpart of the Genome} \label{approachBa}

The full \ac{SARS-CoV-2} genome consists of 29.904 nucleotides. Due to the limited computing capacity on the local computers, the dimensionality of the dataset had to be reduced. This is done by selecting a subpart strang based on the gained data insights from chapter \ref{experimentsA}. In figure \ref{mutatedGeneticLoci} parts of the genome with lots of mutations are visible. Based upon this 99 nucleotides (33 codons) in the range between position 21.800 and 21.899 were selected. For simplicity, the selected strang sequences are treated as if they were full genome sequences in the following sections.

\subsubsection{Transformation of Genome Sequence to Numeric Model Input} \label{approachBb}

Input to the model are numericalized codon sequences with each codon being composed of three nucleotides (one amino acid). Therefore one needs to build a vocabulary with all codons existing in the dataset that assigns each codon a unique number. Each genome string is therefore tokenized to codons and so far unseen codons are added to the vocabulary. The vocabulary also contains special tokens:

\begin{itemize}
	\item \textit{<SOS>}: start of sentence
	\item \textit{<EOS>}: end of sentence
	\item \textit{<UNK>}: unknown
	\item \textit{<PAD>}: padding
\end{itemize}

A codon is encoded as unknown in the case at least one nucleotide is unknown or at least one padding character is contained. Only in the special case of three padding characters, the padding token is returned. 

To integrate the dataset into the training routine, PyTorchs \textit{DataLoader} is used (see \autoref{approachD}). Therefore a \textit{CustomGISAIDDataset} class is derived from PyTorchs \textit{Dataset} class that can be utilized by the \textit{DataLoader}. It can be configured to return the training or test dataset instances. Each instance is already numericalized by vocabulary index lookups for each codon, padded with the numericalized \textit{<SOS>} and \textit{<EOS>} token and transformed to a PyTorch tensor. 

\subsection{Model Architecture} \label{approachC}

The connection of modeling evolution theory of virus mutations to probabilistic language modeling has already been explained in \autoref{fundamentalsB}. To create such a probabilistic language model an approach closely related to the one introduced by Berman et al. \cite{Berman2020} as explained in \autoref{fundamentalsG} is chosen with the novelty that attention-based transformers are used instead of \acp{LSTM}\footnote{This improvement was also named in the conclusion of the MutaGAN paper \cite{Berman2020}.}. To implement the \ac{GAN} framework, PyTorch version 1.9.0 is used as it provides full-featured classes for transformers, embedding layers and other architectural components needed compared to other frameworks like Keras. This way one can specialize on the training routine instead of implementing a custom transformer architecture, which might be too error-prone\footnote{In case one is interested in how to implement a custom transformer architecture, see \url{http://nlp.seas.harvard.edu/2018/04/03/attention.html}.}. 

The architecture of the generator was shown in \autoref{transformer}. The numericalized codon sequences of the source parent and the target child genome sequence first need to be transformed to input embeddings of size 32 for each codon. This size was chosen experimentally and seemed to work best in practice as it only needs to model a vocabulary of size 40 for the parent codon vocabulary and 40 for the child codon vocabulary. To those input embeddings, positional encodings are added that are formed by embedding each index in a sequence of positional indices for each codon in a genome instance. To both input sequences dropout layers are applied with a dropout rate of 10\%. The transformer then predicts the child genome sequence as a hidden representation of dimensionality 64 for each codon. The hidden representation is the output of the feed-forward layer in a transformer block, its dimensionality was also chosen according to best experimental performance. In total three encoder and decoder layers are used each having eight heads. The transformer also uses a dropout rate of 10\%. The final feed-forward network reduces the dimensionality from 64 back to the size of the vocabulary for each codon and applies the softmax function to receive the final output probabilities. Using greedy decoding the codon with the highest probability is returned in the evaluation phase as well as during training for simplicity reasons (instead of using a beam search approach). Note that padding tokens or so far unpredicted child tokens are masked away to not be considered when calculating the loss during the training routine.

The architecture of the discriminator is kept similar to \autoref{mutagan} using PyTorch's \textit{TransformerEncoders} instead of \acp{LSTM}. Again one transforms the numericalized parent and either true or generated child genome sequence to embeddings of size 32 for each codon and adds the positional embeddings before applying dropout. Instead of using an embedding layer for the true or generated child genome sequence, one uses a dense layer instead, to directly feed in the probability distribution of generated child sequences. The true child sequence therefore needs to be one-hot encoded before being provided to the discriminator to match the shape. The transformer encoder then produces an encoding of dimensionality 64 for each codon. During the training phase dropout is once again used with a rate of 10\%. Note that until this step the same weights are used as in the generator. In the following step, the two hidden representations of the parent and true or generated child genome sequence are concatenated along the embedding dimension. The subsequent feed-forward network uses three blocks of dropout, batch normalization and a linear layer in this order with \ac{ReLU} activation. The \ac{ReLU} layer in the last block is replaced with a sigmoid layer that makes sure each codon receives a score between zero and one that states how likely its a true or generated codon. Once again padding tokens are masked away during the training phase.

\subsection{Training Process} \label{approachD}

The training phase is two-fold and is performed on a single GeForce GTX 1080 \ac{GPU}. First, a pretraining phase for the generator needs to be performed in order to produce the first reasonable child genome sequences. Also, the weights of the embedding layers and the encoder part of the generator will be reused for the discriminator in the second training phase. For the pretraining phase, the cross-entropy loss function typical for language modeling is used, the learning rate is set to 0.005. Teacher forcing is used in the initial pretraining phase to accelerate the training process by leading the generator to the right direction towards reasonable child genome sequences. In the main training phase, the generator and discriminator compete against each other. When training the discriminator, the generator first generates a child genome sequence given a parent genome sequence. The true and generated child genome sequence is then given to the discriminator. Here the binary cross-entropy loss is used to model the two parts of the classical \ac{GAN} loss function: 

\begin{equation}
	\underset{G}{arg min} \; \underset{D}{arg max} \; E_{x \sim p*(x)}(log(D(x))) + E_{z \sim p(z)}(log(1-D(G(z))))
\end{equation}

PyTorch's \textit{BCELoss} function is given as:

\begin{equation}
	l = -w(y\,log(x)) + (1-y)\,log(1-x)
\end{equation}

We therefore pass $y=1$ when calculating the left part of the loss using the true child genome sequence and $y=0$ when calculating the right part of the loss using the generated child genome sequence. We use the same mechanism to calculate the loss for the generator, but maximize $log(D(G(z)))$ instead of minimizing $log(1-D(G(z)))$. After experiencing mode collapse problems Berman et al. \cite{Berman2020} switched to the Wasserstein loss function instead of the binary cross-entropy function and therefore also changed the last layer of the discriminator to be a linear activation function. As no mode collapse was discovered and PyTorch does not offer a Wasserstein loss function the binary cross-entropy loss function was kept. The learning rate for the generator was set to 0.0001 whereas the learning rate for the discriminator was set far lower to 0.00003 to avoid mode collapse. Besides providing true child genome sequences and generated child genome sequences to the discriminator, Berman et al. \cite{Berman2020} also provide unrelated, but true child genome sequences to the discriminator. This way it is hoped to not only learn which mutations are relevant as inherently given by the training dataset but also to strengthen the knowledge of which mutations are especially related to a specific parent genome sequence and which not. For time reasons the training dataset was not enriched with such true but unrelated child genome sequences. 

Both training phases use batch gradient descent as their optimization method with a batch size of 32. Gradient descent is performed alternatingly between the generator and discriminator on every epoch. Gradients are clipped to a maximal norm of one to avoid exploding gradients. Furthermore, for both training phases, the Adam optimizer is used. A learning rate scheduler makes sure to reduce the learning rate by a factor of ten in case the loss has not improved for ten epochs. On every batch gradient descent, a tensorboard is updated to draw the loss curve. Every ten epochs a model checkpoint is saved. Pretraining is done for only 50 epochs whereas the main training phase runs for 300 epochs. 

\newpage
