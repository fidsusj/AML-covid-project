\section{Fundamentals and Related Work} \label{fundamentals}

\subsection{From Language Models to modeling Evolution Theory} \label{fundamentalsA}

Using such a seq2seq model a connection to the domain of neural machine translation can be drawn that changes the meaning of sentences but preserves a specific grammar (cf. preserve infectivity and evolutionary fitness but avoid the detection by the human immune system).

\subsection{GISAID EpiFlu} \label{fundamentalsB}

\subsection{Domain-Specific Methodologies to create Evolutionary  Datasets for Mutation Prediction} \label{fundamentalsC}

\subsection{Domain-Specific Methodologies to create Evolutionary  Datasets for Mutation Prediction} \label{fundamentalsD}

\subsection{Previous work on Mutation Prediction} \label{fundamentalsE}

% TODO: To be continued with the other papers from the proposal

Even before the rise of Covid-19 there had been studies trying to predict mutations of RNA viruses. In the collection of \cite{Wu2007, Yan2007, Wu2008} the authors predict the mutation positions in hemagglutinins from influenza A virus using logistic regression and plain neural networks and then use the resulting amino acid mutating probabilities to derive possible mutated amnio acids. The same approach was further used for H5N1 neuraminidase proteins. 

\cite{Salama2016} proved that nucleotides in an RNA sequence can change based on their local neighborhood. Neural networks are used to predict new strains of the Newcastle virus and subsequently a rough set theory based algorithm is introduced to extract the according point mutation patterns. 

\cite{Mohamed2021} uses a more modern seq2seq LSTM neural network approach to learn nucleotide mutations between time-series species of H1N1 Influenza virus and the Newcastle virus as mutations can also be influenced by long-distance relations of amino acids. Therefore one hot-encoded RNA sequences of a parent generation preprocessed to words is given as input and the output is the predicted offspring generation evaluated by accuracy to the true offspring generation. The achieved accuracy in this paper is questionably high with 98.9\% on the H1N1 Influenza virus and 96.9\% on the Newcastle virus, possibly because of overfitting to the few 4.609 samples for H1N1 Influenza virus and only 83 for the Newcastle virus. Our approach therefore tries to increase the number of samples available for training when building the dataset. 

Our approach will neither use any of the just mentioned architectures, but uses a Transformer based architecture coupled with a GAN-style training architecture. Nevertheless we would like to give a short introduction into sequence to sequence models and the underlying long short-term memory components to better point out our architectural decisions . 

\subsection{Sequence2Sequence Models based on Long Short-Term Memory} \label{fundamentalsF}

% Use Keras for implementation + Tensorflow 2
% Plot accuracy and loss for training and validation

The original \ac{LSTM} unit was introduced in \cite{Hochreiter1997} and can be used for language modeling instead of using plain \acp{RNN} to prevent running into vanishing gradient problems \cite{Sundermeyer2012}. 

% TODO: LSTM cell architecture video, long range dependencies

% Add a figure showing that it is one timestep at the time
% How does beam search work?


\cite{bla} introduced sequence to sequence learning by using one LSTM to learn a large fixed-dimensional vector representation of the input that is provided one timestamp at the time and another \ac{LSTM} to map the so-called context vector to a corresponding output sequence whose length does not need to match with the length of the input sequence. The output sequence is therefore given by the equation

\begin{equation}
	p(y_1, ..., y_{T'} | x_1, ..., x_{T}) = \Pi_{t=1}^{T'} p(y_t | v, y_1, ..., y_{t-1})
\end{equation}

where each $p(y_t | v, y_1, ..., y_{t-1})$ is given by the softmax over all words in the vocabulary. Using an \ac{LSTM} is prefered over a normal \ac{RNN} as it is used to capture the long range temporal dependencies of the input data, each \ac{LSTM} uses four layers with 1000 cells each. One finding was also that reversing the input sequence introduces many short term dependencies making optimization easier. The sequence to sequence model approach was evaluated for neural machine translation and reached a 34.81 BLEU score with an output vocabulary of 80k words (160k words input vocabulary, 1000 dimensional word embeddings, 8000 real numbers to represent a sentence). 

% From Hindawi: Each LSTM cell takes information as input from the previous cell, in the form of a hidden state vector and cell state vector (represented in arrows) and combines it with the one-hot encoded vector. The output of the encoder is the concatenation of the hidden and cell-state vectors. In the decoder, each cell receives as input [i] in the one-hot encoded version and the previous word of the next generation generated by the model, as well as the hidden state and cell-state vectors from the previous cell. It not only passes those two vectors after updating them to the next cell but also feeds the hidden state vector to the dense layer (output layer), which outputs a probability distribution over the next generation word at that position. A SoftMax function is applied to obtain probability distribution. To reduce overfitting, early stopping is used to end training when the validation loss did not decrease.

---

\begin{itemize}
	\item LSTM: \url{https://www.researchgate.net/publication/13853244_Long_Short-term_Memory}
	\item LSTM: \url{https://www.isca-speech.org/archive/archive_papers/interspeech_2012/i12_0194.pdf}
	\item Seq2Seq: \url{https://arxiv.org/abs/1409.3215}
\end{itemize}


\subsection{Applying Generative Adversarial Networks} \label{fundamentalsG}

\begin{itemize}
	\item Covid-Paper: \url{https://arxiv.org/pdf/2008.11790.pdf}
\end{itemize}


\subsection{Transformer and Attention Mechanism} \label{fundamentalsH}

\begin{itemize}
	\item Improvement: \url{https://arxiv.org/abs/1706.03762}
\end{itemize}


\subsection{Other Techniques} \label{fundamentalsI}

\begin{itemize}
	\item NNs/SVMs: \url{https://bsb-eurasipjournals.springeropen.com/articles/10.1186/s13637-016-0042-0}
	\item BiLSTM: \url{https://science.sciencemag.org/content/371/6526/284}
\end{itemize}


\newpage
